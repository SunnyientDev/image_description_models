# Image description models

## BLIP-2

### ARTICLE
[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
Problem type: image_to_text

BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model.

Sources: [huggingface](https://huggingface.co/docs/transformers/model_doc/blip-2)

### EXPLANATION 
Notion [ru]: [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://attractive-tadpole-aa0.notion.site/BLIP-2-a897915ea71d4ac8900783dd074d0e23?pvs=4)
